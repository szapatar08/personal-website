# Problema
Las empresas entrenan modelos de IA con datos contaminados sin saberlo, lo que incrementa costos, retrasa el desarrollo y pone en riesgo la seguridad y confiabilidad de los modelos.

## Pregunta problematizadora
¿Y si pudiéramos detectar y eliminar datos maliciosos antes del entrenamiento para ahorrar tiempo, dinero y construir modelos de IA más seguros y sostenibles?

## Propuesta de valor
Poison AI detecta, clasifica y elimina datos contaminados o maliciosos antes del entrenamiento y fine-tuning de modelos de IA, permitiendo a las empresas reducir costos, ahorrar tiempo y construir modelos más seguros, confiables y sostenibles.

Valor clave que entregamos:
   - Menos costo en entrenamiento y retraining innecesario
   - Menos tiempo perdido por datasets defectuosos
   - Más seguridad frente a data poisoning y ataques silenciosos
   - Más sostenibilidad al evitar cómputo desperdiciado
   - Mejor performance en modelos generativos y ML en producción

Para quién:
Empresas que entrenan o hacen fine-tuning de modelos de IA.
